{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T14:26:53.297128Z",
     "start_time": "2023-06-05T14:26:53.286822Z"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ\n",
    "os.environ['PYTHONUNBUFFERED'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVIC'] = '0'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "os.environ['INHERIT_BERT'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T14:26:57.432687Z",
     "start_time": "2023-06-05T14:26:53.299853Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexch/opt/anaconda3/envs/dragon/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedModelClass <class 'transformers.models.bert.modeling_bert.BertPreTrainedModel'>\n",
      "ModelClass <class 'transformers.models.bert.modeling_bert.BertModel'>\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import pickle\n",
    "from typing import List\n",
    "\n",
    "# from tensorboardX import SummaryWriter\n",
    "import torch\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "\n",
    "from modeling.modeling_dragon import DRAGON\n",
    "from utils.data_utils import Custom_DataLoader, DRAGON_DataLoader, simple_convert_examples_to_features, InputExample, \\\n",
    "    MODEL_NAME_TO_CLASS, InputFeatures\n",
    "\n",
    "try:\n",
    "    from transformers import (ConstantLRSchedule, WarmupLinearSchedule, WarmupConstantSchedule, BertTokenizer,\n",
    "                              AlbertTokenizer, XLNetTokenizer, RobertaTokenizer)\n",
    "except:\n",
    "    from transformers import get_constant_schedule, get_constant_schedule_with_warmup, get_linear_schedule_with_warmup, \\\n",
    "        BertTokenizer\n",
    "import wandb\n",
    "from transformers import (OpenAIGPTTokenizer, BertTokenizer, BertTokenizerFast, XLNetTokenizer, RobertaTokenizer,\n",
    "                          RobertaTokenizerFast)\n",
    "from modeling import modeling_dragon\n",
    "from utils import utils\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import socket, os, sys, subprocess\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Custom_DataLoader(DRAGON_DataLoader):\n",
    "    def __init__(self, question: str, answers: List[str], args, train_statement_path, train_adj_path,\n",
    "                 dev_statement_path, dev_adj_path,\n",
    "                 test_statement_path, test_adj_path,\n",
    "                 batch_size, eval_batch_size, device, model_name, max_node_num=200, max_seq_length=128,\n",
    "                 is_inhouse=False, inhouse_train_qids_path=None,\n",
    "                 subsample=1.0, n_train=-1, debug=False, cxt_node_connects_all=False, kg=\"cpnet\"):\n",
    "        self.args = args\n",
    "        self.batch_size = batch_size\n",
    "        self.eval_batch_size = eval_batch_size\n",
    "        self.device0, self.device1 = device\n",
    "        self.is_inhouse = is_inhouse\n",
    "        self.debug = debug\n",
    "        self.model_name = model_name\n",
    "        self.max_node_num = max_node_num\n",
    "        self.debug_sample_size = 32\n",
    "        self.cxt_node_connects_all = cxt_node_connects_all\n",
    "\n",
    "        self.model_type = MODEL_NAME_TO_CLASS[model_name]\n",
    "        self.load_resources(kg)\n",
    "\n",
    "        # Load training data\n",
    "        print('train_statement_path', train_statement_path)\n",
    "        self.train_qids, self.train_labels, self.train_encoder_data, train_concepts_by_sents_list = self.load_input_tensors(\n",
    "            question, answers, max_seq_length, mode='train')\n",
    "\n",
    "        num_choice = self.train_encoder_data[0].size(1)\n",
    "        self.num_choice = num_choice\n",
    "        print('num_choice', num_choice)\n",
    "        *self.train_decoder_data, self.train_adj_data = self.load_sparse_adj_data_with_contextnode(train_adj_path,\n",
    "                                                                                                   max_node_num,\n",
    "                                                                                                   train_concepts_by_sents_list,\n",
    "                                                                                                   mode='train')\n",
    "\n",
    "    def load_input_tensors(self, question, answers, max_seq_length, mode='eval'):\n",
    "        \"\"\"Construct input tensors for the LM component of the model.\"\"\"\n",
    "\n",
    "        if self.model_type in ('bert', 'xlnet', 'roberta', 'albert'):\n",
    "            # input_tensors = load_bert_xlnet_roberta_input_tensors(input_jsonl_path, max_seq_length, self.debug, self.tokenizer, self.debug_sample_size)\n",
    "            input_tensors = load_bert_xlnet_roberta_input_from_text(question, answers, max_seq_length, self.debug,\n",
    "                                                                    self.tokenizer, self.debug_sample_size)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        if mode == 'train' and self.args.local_rank != -1:\n",
    "            example_ids, all_label, data_tensors, concepts_by_sents_list = input_tensors  # concepts_by_sents_list is always []\n",
    "            assert len(example_ids) == len(all_label) == len(data_tensors[0])\n",
    "            total_num = len(data_tensors[0])\n",
    "            rem = total_num % self.args.world_size\n",
    "            if rem != 0:\n",
    "                example_ids = example_ids + example_ids[:self.args.world_size - rem]\n",
    "                all_label = torch.cat([all_label, all_label[:self.args.world_size - rem]], dim=0)\n",
    "                data_tensors = [torch.cat([t, t[:self.args.world_size - rem]], dim=0) for t in data_tensors]\n",
    "                total_num_aim = total_num + self.args.world_size - rem\n",
    "            else:\n",
    "                total_num_aim = total_num\n",
    "            assert total_num_aim % self.args.world_size == 0\n",
    "            assert total_num_aim == len(data_tensors[0])\n",
    "            _select = (torch.arange(total_num_aim) % self.args.world_size) == self.args.local_rank  # bool tensor\n",
    "            example_ids = np.array(example_ids)[_select].tolist()\n",
    "            all_label = all_label[_select]\n",
    "            data_tensors = [t[_select] for t in data_tensors]\n",
    "            input_tensors = (example_ids, all_label, data_tensors, [])\n",
    "        example_ids = input_tensors[0]\n",
    "        print('local_rank', self.args.local_rank, 'len(example_ids)', len(example_ids), file=sys.stderr)\n",
    "        return input_tensors\n",
    "\n",
    "\n",
    "def load_bert_xlnet_roberta_input_from_text(\n",
    "        contexts: str,  # = \"A 23-year-old pregnant woman at 22 weeks gestation presents wit\",\n",
    "        answers: List[str],  # = ['Ampicillin', 'Ceftriaxone', 'Doxycycline', 'Nitrofurantoin'],\n",
    "        max_seq_length, debug, tokenizer, label=3):\n",
    "    def select_field(features, field):\n",
    "        return [[choice[field] for choice in feature.choices_features] for feature in features]\n",
    "\n",
    "    def convert_features_to_tensors(features):\n",
    "        all_input_ids = torch.tensor(select_field(features, 'input_ids'), dtype=torch.long)\n",
    "        all_input_mask = torch.tensor(select_field(features, 'input_mask'), dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor(select_field(features, 'segment_ids'), dtype=torch.long)\n",
    "        all_output_mask = torch.tensor(select_field(features, 'output_mask'), dtype=torch.bool)\n",
    "        all_label = torch.tensor([f.label for f in features], dtype=torch.long)\n",
    "        return all_input_ids, all_input_mask, all_segment_ids, all_output_mask, all_label\n",
    "\n",
    "    examples = [InputExample(\n",
    "        example_id=\"train-00000\",\n",
    "        contexts=[contexts] * len(answers),\n",
    "        question=\"\",\n",
    "        endings=answers,\n",
    "        label=label\n",
    "    )]\n",
    "    features, concepts_by_sents_list = simple_convert_examples_to_features(examples,\n",
    "                                                                           list(range(len(examples[0].endings))),\n",
    "                                                                           max_seq_length, tokenizer, debug)\n",
    "\n",
    "    example_ids = [f.example_id for f in features]\n",
    "    *data_tensors, all_label = convert_features_to_tensors(features)\n",
    "    return example_ids, all_label, data_tensors, concepts_by_sents_list\n",
    "\n",
    "\n",
    "def simple_convert_examples_to_features(examples, label_list, max_seq_length, tokenizer, debug=False):\n",
    "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
    "        `cls_token_at_end` define the location of the CLS token:\n",
    "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
    "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
    "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
    "    \"\"\"\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    concepts_by_sents_list = []\n",
    "    for ex_index, example in tqdm(enumerate(examples), total=len(examples), desc=\"Converting examples to features\"):\n",
    "        if debug and ex_index >= 32:\n",
    "            break\n",
    "        choices_features = []\n",
    "        for ending_idx, (context, ending) in enumerate(zip(example.contexts, example.endings)):\n",
    "            ans = example.question + \" \" + ending\n",
    "\n",
    "            encoded_input = tokenizer(context, ans, padding=\"max_length\", truncation=True, max_length=max_seq_length,\n",
    "                                      return_token_type_ids=True, return_special_tokens_mask=True)\n",
    "            input_ids = encoded_input[\"input_ids\"]\n",
    "            output_mask = encoded_input[\"special_tokens_mask\"]\n",
    "            input_mask = encoded_input[\"attention_mask\"]\n",
    "            segment_ids = encoded_input[\"token_type_ids\"]\n",
    "\n",
    "            assert len(input_ids) == max_seq_length\n",
    "            assert len(output_mask) == max_seq_length\n",
    "            assert len(input_mask) == max_seq_length\n",
    "            assert len(segment_ids) == max_seq_length\n",
    "\n",
    "            choices_features.append((input_ids, input_mask, segment_ids, output_mask))\n",
    "        label = label_map.get(example.label, -100)\n",
    "        features.append(InputFeatures(example_id=example.example_id, choices_features=choices_features, label=label))\n",
    "\n",
    "    return features, concepts_by_sents_list\n",
    "\n",
    "\n",
    "def construct_model(args, kg, dataset_final_num_relation=100):\n",
    "    ########################################################\n",
    "    #   Load pretrained concept embeddings\n",
    "    ########################################################\n",
    "    cp_emb = [np.load(path) for path in args.ent_emb_paths]\n",
    "    cp_emb = np.concatenate(cp_emb, 1)\n",
    "    cp_emb = torch.tensor(cp_emb, dtype=torch.float)\n",
    "\n",
    "    concept_num, concept_in_dim = cp_emb.size(0), cp_emb.size(1)\n",
    "    print('| num_concepts: {} |'.format(concept_num))\n",
    "    if args.random_ent_emb:\n",
    "        cp_emb = None\n",
    "        freeze_ent_emb = False\n",
    "        concept_in_dim = args.gnn_dim\n",
    "    else:\n",
    "        freeze_ent_emb = args.freeze_ent_emb\n",
    "\n",
    "    ##########################################################\n",
    "    #   Build model\n",
    "    ##########################################################\n",
    "\n",
    "    if kg == \"cpnet\":\n",
    "        n_ntype = 4\n",
    "        n_etype = 38\n",
    "        # assert n_etype == dataset.final_num_relation *2\n",
    "    elif kg == \"ddb\":\n",
    "        n_ntype = 4\n",
    "        n_etype = 34\n",
    "        # assert n_etype == dataset.final_num_relation *2\n",
    "    elif kg == \"umls\":\n",
    "        n_ntype = 4\n",
    "        n_etype = dataset_final_num_relation * 2\n",
    "        # print('final_num_relation', dataset.final_num_relation, 'len(id2relation)', len(dataset.id2relation))\n",
    "        # print('final_num_relation', dataset.final_num_relation, 'len(id2relation)', len(dataset.id2relation),\n",
    "        #       file=sys.stderr)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid KG.\")\n",
    "    if args.cxt_node_connects_all:\n",
    "        n_etype += 2\n",
    "    print('n_ntype', n_ntype, 'n_etype', n_etype)\n",
    "    print('n_ntype', n_ntype, 'n_etype', n_etype, file=sys.stderr)\n",
    "    encoder_load_path = args.encoder_load_path if args.encoder_load_path else args.encoder\n",
    "    model = modeling_dragon.DRAGON(args, encoder_load_path, k=args.k, n_ntype=n_ntype, n_etype=n_etype,\n",
    "                                   n_concept=concept_num,\n",
    "                                   concept_dim=args.gnn_dim,\n",
    "                                   concept_in_dim=concept_in_dim,\n",
    "                                   n_attention_head=args.att_head_num, fc_dim=args.fc_dim, n_fc_layer=args.fc_layer_num,\n",
    "                                   p_emb=args.dropouti, p_gnn=args.dropoutg, p_fc=args.dropoutf,\n",
    "                                   pretrained_concept_emb=cp_emb, freeze_ent_emb=freeze_ent_emb,\n",
    "                                   init_range=args.init_range, ie_dim=args.ie_dim, info_exchange=args.info_exchange,\n",
    "                                   ie_layer_num=args.ie_layer_num, sep_ie_layers=args.sep_ie_layers,\n",
    "                                   layer_id=args.encoder_layer)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_pred(question: str, answers: List[str], model: DRAGON):\n",
    "    \"\"\"Eval on the dev or test set - calculate loss and accuracy\"\"\"\n",
    "\n",
    "    eval_set = Custom_DataLoader(question, answers, args, args.train_statements, args.train_adj,\n",
    "                                 args.dev_statements, args.dev_adj,\n",
    "                                 args.test_statements, args.test_adj,\n",
    "                                 batch_size=args.batch_size, eval_batch_size=args.eval_batch_size,\n",
    "                                 device=devices,\n",
    "                                 model_name=args.encoder,\n",
    "                                 max_node_num=args.max_node_num, max_seq_length=args.max_seq_len,\n",
    "                                 is_inhouse=args.inhouse, inhouse_train_qids_path=args.inhouse_train_qids,\n",
    "                                 subsample=args.subsample, n_train=args.n_train, debug=args.debug,\n",
    "                                 cxt_node_connects_all=args.cxt_node_connects_all, kg=kg)  # .train()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for qids, labels, *input_data in tqdm(eval_set.train(), desc=\"Dev/Test batch\"):\n",
    "            logits, mlm_loss, link_losses = model(*input_data)\n",
    "            predictions = logits.argmax(1)  # [bsize, ]\n",
    "            for qid, pred in zip(qids, predictions):\n",
    "                return '{},{}'.format(qid, chr(ord('A') + pred.item())), pred\n",
    "\n",
    "\n",
    "def get_model(args, devices, kg) -> DRAGON:\n",
    "    question = \"A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\"\n",
    "    answers = [\"Nitrofurantoin\", \"Ampicillin\", \"Ceftriaxone\", \"Doxycycline\"]\n",
    "    assert args.load_model_path is not None\n",
    "    load_model_path = args.load_model_path\n",
    "    print(\"loading from checkpoint: {}\".format(load_model_path))\n",
    "    checkpoint = torch.load(load_model_path, map_location='cpu')\n",
    "\n",
    "    train_statements = args.train_statements\n",
    "    dev_statements = args.dev_statements\n",
    "    test_statements = args.test_statements\n",
    "    train_adj = args.train_adj\n",
    "    dev_adj = args.dev_adj\n",
    "    test_adj = args.test_adj\n",
    "    inhouse = args.inhouse\n",
    "\n",
    "    # args = utils.import_config(checkpoint[\"config\"], args)\n",
    "    args.train_statements = train_statements\n",
    "    args.dev_statements = dev_statements\n",
    "    args.test_statements = test_statements\n",
    "    args.train_adj = train_adj\n",
    "    args.dev_adj = dev_adj\n",
    "    args.test_adj = test_adj\n",
    "    args.inhouse = inhouse\n",
    "\n",
    "    # dataset = Custom_DataLoader(question, answers, args, args.train_statements, args.train_adj,\n",
    "    #                             args.dev_statements, args.dev_adj,\n",
    "    #                             args.test_statements, args.test_adj,\n",
    "    #                             batch_size=args.batch_size, eval_batch_size=args.eval_batch_size,\n",
    "    #                             device=devices,\n",
    "    #                             model_name=args.encoder,\n",
    "    #                             max_node_num=args.max_node_num, max_seq_length=args.max_seq_len,\n",
    "    #                             is_inhouse=args.inhouse, inhouse_train_qids_path=args.inhouse_train_qids,\n",
    "    #                             subsample=args.subsample, n_train=args.n_train, debug=args.debug,\n",
    "    #                             cxt_node_connects_all=args.cxt_node_connects_all, kg=kg)\n",
    "\n",
    "    model = construct_model(args, kg)\n",
    "    INHERIT_BERT = os.environ.get('INHERIT_BERT', 0)\n",
    "    bert_or_roberta = model.lmgnn.bert if INHERIT_BERT else model.lmgnn.roberta\n",
    "    try:\n",
    "        tokenizer_class = {'bert': BertTokenizer, 'xlnet': XLNetTokenizer, 'roberta': RobertaTokenizer,\n",
    "                           'albert': AlbertTokenizer}.get(MODEL_NAME_TO_CLASS[args.encoder])\n",
    "    except:\n",
    "        tokenizer_class = {'bert': BertTokenizer, 'xlnet': XLNetTokenizer, 'roberta': RobertaTokenizer}.get(\n",
    "            MODEL_NAME_TO_CLASS[args.encoder])\n",
    "    tokenizer = tokenizer_class.from_pretrained(args.encoder)\n",
    "\n",
    "    bert_or_roberta.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"model\"], strict=False)\n",
    "\n",
    "    model.to(devices[1])\n",
    "    model.lmgnn.concept_emb.to(devices[0])\n",
    "    model.eval()\n",
    "\n",
    "    print('inhouse?', args.inhouse)\n",
    "\n",
    "    print('args.train_statements', args.train_statements)\n",
    "    print('args.dev_statements', args.dev_statements)\n",
    "    print('args.test_statements', args.test_statements)\n",
    "    print('args.train_adj', args.train_adj)\n",
    "    print('args.dev_adj', args.dev_adj)\n",
    "    print('args.test_adj', args.test_adj)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_devices(args):\n",
    "    \"\"\"Get the devices to put the data and the model based on whether to use GPUs and, if so, how many of them are available.\"\"\"\n",
    "\n",
    "    if args.local_rank == -1 or not args.cuda:\n",
    "        if torch.cuda.device_count() >= 2 and args.cuda:\n",
    "            device0 = torch.device(\"cuda:0\")\n",
    "            device1 = torch.device(\"cuda:1\")\n",
    "            print(\"device0: {}, device1: {}\".format(device0, device1))\n",
    "        elif torch.cuda.device_count() == 1 and args.cuda:\n",
    "            device0 = torch.device(\"cuda:0\")\n",
    "            device1 = torch.device(\"cuda:0\")\n",
    "        else:\n",
    "            device0 = torch.device(\"cpu\")\n",
    "            device1 = torch.device(\"cpu\")\n",
    "    else:\n",
    "        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        device0 = torch.device(\"cuda\", args.local_rank)\n",
    "        device1 = device0\n",
    "        torch.distributed.init_process_group(backend=\"nccl\")\n",
    "\n",
    "    args.world_size = world_size = torch.distributed.get_world_size() if args.local_rank != -1 else 1\n",
    "    print(\"Process rank: %s, device: %s, distributed training: %s, world_size: %s\" %\n",
    "          (args.local_rank,\n",
    "           device0,\n",
    "           bool(args.local_rank != -1),\n",
    "           world_size), file=sys.stderr)\n",
    "\n",
    "    return device0, device1\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s,%(msecs)d %(levelname)-8s [%(name)s:%(funcName)s():%(lineno)d] %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T14:27:18.228226Z",
     "start_time": "2023-06-05T14:26:57.435392Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args = argparse.Namespace(att_head_num=2, batch_size=1, cuda=True, cxt_node_connects_all=False, data_dir='data',\n",
    "                          data_loader_one_process_at_a_time=False, dataset='medqa', debug=True, decoder_lr=0.0001,\n",
    "                          dev_adj='data/medqa/graph/dev.graph.adj.pk',\n",
    "                          dev_statements='data/medqa/statement/dev.statement.jsonl', dropoutf=0.2, dropoutg=0.2,\n",
    "                          dropouti=0.2, dump_graph_cache=True, encoder='michiyasunaga/BioLinkBERT-large',\n",
    "                          encoder_layer=-1, encoder_load_path='', encoder_lr=2e-05, end_task=1.0,\n",
    "                          ent_emb_paths=['umls/ent_emb_blbertL.npy'], eval_batch_size=2, eval_interval=5,\n",
    "                          fc_dim=200, fc_layer_num=0, fp16=True, freeze_ent_emb=True, gnn_dim=200, ie_dim=400,\n",
    "                          ie_layer_num=1, info_exchange=True, inhouse=False,\n",
    "                          inhouse_train_qids='data/medqa/inhouse_split_qids.txt', init_range=0.02, k=5, kg='umls',\n",
    "                          kg_only_use_qa_nodes=False, kg_vocab_path='umls/concepts.txt', link_decoder='DistMult',\n",
    "                          link_drop_max_count=100, link_drop_probability=0.2,\n",
    "                          link_drop_probability_in_which_keep=0.2, link_gamma=12,\n",
    "                          link_negative_adversarial_sampling=True, link_negative_adversarial_sampling_temperature=1,\n",
    "                          link_negative_sample_size=64, link_normalize_headtail=0, link_proj_headtail=False,\n",
    "                          link_regularizer_weight=0.01, link_task=0.0, load_graph_cache=True,\n",
    "                          load_model_path='models/medqa_model.pt', local_rank=-1, log_interval=1,\n",
    "                          loss='cross_entropy', lr_schedule='warmup_linear', max_epochs_before_stop=100,\n",
    "                          max_grad_norm=1.0, max_node_num=200, max_num_relation=-1, max_seq_len=512,\n",
    "                          mini_batch_size=1, mlm_probability=0.15, mlm_task=0.0, mode='eval', n_epochs=30,\n",
    "                          n_train=-1, no_node_score=True, optim='radam', random_ent_emb=False, redef_epoch_steps=-1,\n",
    "                          refreeze_epoch=10000, residual_ie=2, resume_checkpoint='None', resume_id='None',\n",
    "                          run_name='run1', save_dir='./saved_models/', save_model=0.0, scaled_distmult=False,\n",
    "                          seed=22, sep_ie_layers=False, span_mask=False, subsample=1.0,\n",
    "                          test_adj='data/medqa/graph/test.graph.adj.pk',\n",
    "                          test_statements='data/medqa/statement/test.statement.jsonl',\n",
    "                          train_adj='data/medqa/graph/train.graph.adj.pk',\n",
    "                          train_statements='data/medqa/statement/train.statement.jsonl', unfreeze_epoch=0,\n",
    "                          upcast=True, use_codalab=0, use_wandb=False, warmup_steps=500.0, weight_decay=0.01,\n",
    "                          world_size=1)\n",
    "\n",
    "devices = get_devices(args)\n",
    "\n",
    "if not args.use_wandb:\n",
    "    wandb_mode = \"disabled\"\n",
    "elif args.debug:\n",
    "    wandb_mode = \"offline\"\n",
    "else:\n",
    "    wandb_mode = \"online\"\n",
    "\n",
    "# We can optionally resume training from a checkpoint. If doing so, also set the `resume_id` so that you resume your previous wandb run instead of creating a new one.\n",
    "resume = args.resume_checkpoint not in [None, \"None\"]\n",
    "\n",
    "args.hf_version = transformers.__version__\n",
    "\n",
    "if args.local_rank in [-1, 0]:\n",
    "    wandb_id = args.resume_id if resume and (args.resume_id not in [None, \"None\"]) else wandb.util.generate_id()\n",
    "    args.wandb_id = wandb_id\n",
    "    wandb.init(project=\"DRAGON\", config=args, name=args.run_name, resume=\"allow\", id=wandb_id,\n",
    "               settings=wandb.Settings(start_method=\"fork\"), mode=wandb_mode)\n",
    "    print(socket.gethostname())\n",
    "    print(\"pid:\", os.getpid())\n",
    "    print(\"conda env:\", os.environ.get('CONDA_DEFAULT_ENV'))\n",
    "    print(\"screen: %s\" % subprocess.check_output('echo $STY', shell=True).decode('utf'))\n",
    "    print(\"gpu: %s\" % subprocess.check_output('echo $CUDA_VISIBLE_DEVICES', shell=True).decode('utf'))\n",
    "    utils.print_cuda_info()\n",
    "    print(\"wandb id: \", wandb_id)\n",
    "    wandb.run.log_code('.')\n",
    "\n",
    "kg = args.kg\n",
    "if args.dataset == \"medqa_usmle\":\n",
    "    kg = \"ddb\"\n",
    "elif args.dataset in [\"medqa\", \"pubmedqa\", \"bioasq\"]:\n",
    "    kg = \"umls\"\n",
    "print(\"KG used:\", kg)\n",
    "print(\"KG used:\", kg, file=sys.stderr)\n",
    "model = get_model(args, devices, kg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T14:27:44.362380Z",
     "start_time": "2023-06-05T14:27:18.234449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_statement_path data/medqa/statement/train.statement.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting examples to features: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 75.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_choice 4\n",
      "Loading sparse adj data...\n",
      "Loading cache data/medqa/graph/train.graph.adj.pk-nodenum200.loaded_cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "local_rank -1 len(example_ids) 1\n",
      "Loading cache data/medqa/graph/train.graph.adj.pk-nodenum200.loaded_cache\n",
      "Loaded cache data/medqa/graph/train.graph.adj.pk-nodenum200.loaded_cache\n",
      "local_rank -1 len(edge_index) 10178\n",
      "local_rank -1 len(train_indexes) 32 train_indexes[:10] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ori_adj_len: mu 297.77 sigma 264.97 | adj_len: 147.16 | prune_rate： 0.53 | qc_num: 28.09 | ac_num: 1.54 |\n",
      "local_rank -1 len(train_indexes) 32 train_indexes[:10] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev/Test batch:   0%|                                                                                                                                           | 0/32 [00:15<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer is: 4\n"
     ]
    }
   ],
   "source": [
    "question = \"A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\"\n",
    "answers = [\"Ampicillin\", \"Ceftriaxone\", \"Doxycycline\", \"Nitrofurantoin\"]\n",
    "preds = get_pred(question, answers, model)\n",
    "print(f\"Answer is: {preds[1].item() + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T14:28:09.181831Z",
     "start_time": "2023-06-05T14:27:44.367993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_statement_path data/medqa/statement/train.statement.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting examples to features: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 322.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_choice 4\n",
      "Loading sparse adj data...\n",
      "Loading cache data/medqa/graph/train.graph.adj.pk-nodenum200.loaded_cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "local_rank -1 len(example_ids) 1\n",
      "Loading cache data/medqa/graph/train.graph.adj.pk-nodenum200.loaded_cache\n",
      "Loaded cache data/medqa/graph/train.graph.adj.pk-nodenum200.loaded_cache\n",
      "local_rank -1 len(edge_index) 10178\n",
      "local_rank -1 len(train_indexes) 32 train_indexes[:10] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ori_adj_len: mu 297.77 sigma 264.97 | adj_len: 147.16 | prune_rate： 0.53 | qc_num: 28.09 | ac_num: 1.54 |\n",
      "local_rank -1 len(train_indexes) 32 train_indexes[:10] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev/Test batch:   0%|                                                                                                                                           | 0/32 [00:15<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer is: 2\n"
     ]
    }
   ],
   "source": [
    "question = \"I feel tired in the morning after I wake up. I sleep 6 hours per day. How can I overcome this issue?\"\n",
    "answers = [\n",
    "    \"You should sleep 1 hour more per day.\",\n",
    "    \"You should sleep 2 hours more per day.\",\n",
    "    \"You should sleep 1 hour less per day.\",\n",
    "    \"Your sleep duration is fine. Try to go to bed earlier.\"]\n",
    "preds = get_pred(question, answers, model)\n",
    "print(f\"Answer is: {preds[1].item() + 1}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dragon",
   "language": "python",
   "name": "dragon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
